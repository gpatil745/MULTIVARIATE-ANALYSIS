---
title: "MVA Assignment_5"
output: html_document
---

```{r}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)

data <- read.csv("C:/Users/mumba/Documents/Global_Unemployment_Dataset.csv")

```

```{r}
str(data)
```

```{r}
data$Gender <- as.factor(data$Gender)
data$Age_group <- as.factor(data$Age_group)
data$Age_categories <- as.factor(data$Age_categories)
str(data)
```

# Hierarchical Clustering 
```{r}

# Standardizing the data with scale()
matstd.data <- scale(data[,5:7][,-1])

# Creating a (Euclidean) distance matrix of the standardized data                     
dist.data <- dist(matstd.data, method="euclidean")
colnames(dist.data) <- rownames(dist.data)

# Invoking hclust command (cluster analysis by single linkage method)      
clusdata.nn <- hclust(dist.data, method = "single")

#dendogram

options(repr.plot.width=10, repr.plot.height=6)  # Adjust the plot size as needed
plot(as.dendrogram(clusdata.nn), ylab="Distance between independent variables",
     main="Dendrogram. Unemployment Rate of 3 years")

```

```{r}
# We will use agnes function as it allows us to select option for data standardization, the distance measure and clustering algorithm in one single function
(agn.data <- agnes(data, metric="euclidean", stand=TRUE, method = "single"))

plot(as.dendrogram(agn.data), xlab= "Distance between Countries",xlim=c(8,0),
     horiz = TRUE,main="Dendrogram. Unemployment Rate of 3 years")
```
The hierarchical clustering method Going from top to bottom, I believe the optimal number of clusters is 3. : The dendrogram shows that the clusters split into two groups during the initial break. Two more breaks are then added to the second break i.e Three clusters there. However, four clusters would be ideal. 

# Non-hierarchical clustering/ K-means clustering 
```{r}
# K-means, k=2, 3, 4, 5, 6
# Centers (k's) are numbers thus, 10 random sets are chosen

```

```{r}
# Computing the percentage of variation accounted for. Two clusters
(kmeans2.data <- kmeans(matstd.data,2,nstart = 10))
perc.var.2 <- round(100*(1 - kmeans2.data$betweenss/kmeans2.data$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2
```


```{r}
# Computing the percentage of variation accounted for. Three clusters
(kmeans3.data <- kmeans(matstd.data,3,nstart = 10))
perc.var.3 <- round(100*(1 - kmeans3.data$betweenss/kmeans3.data$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3
```


```{r}
# Computing the percentage of variation accounted for. Four clusters
(kmeans4.data <- kmeans(matstd.data,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.data$betweenss/kmeans4.data$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4
```


```{r}
# Computing the percentage of variation accounted for. Five clusters
(kmeans5.data <- kmeans(matstd.data,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.data$betweenss/kmeans5.data$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5
```


```{r}
# Computing the percentage of variation accounted for. Six clusters
(kmeans6.data <- kmeans(matstd.data,6,nstart = 10))
perc.var.6 <- round(100*(1 - kmeans6.data$betweenss/kmeans6.data$totss),1)
names(perc.var.6) <- "Perc. 6 clus"
perc.var.6
attributes(perc.var.6)
```
```{r}
# Computing the percentage of variation accounted for. Seven clusters
(kmeans7.data <- kmeans(matstd.data,7,nstart = 10))
perc.var.7 <- round(100*(1 - kmeans7.data$betweenss/kmeans7.data$totss),1)
names(perc.var.7) <- "Perc. 7 clus"
perc.var.7
```

```{r}
# Computing the percentage of variation accounted for. Eight clusters
(kmeans8.data <- kmeans(matstd.data,8,nstart = 10))
perc.var.8 <- round(100*(1 - kmeans8.data$betweenss/kmeans8.data$totss),1)
names(perc.var.8) <- "Perc. 8 clus"
perc.var.8
```
```{r}
# Computing the percentage of variation accounted for. Nine clusters
(kmeans9.data <- kmeans(matstd.data,9,nstart = 10))
perc.var.9 <- round(100*(1 - kmeans9.data$betweenss/kmeans9.data$totss),1)
names(perc.var.9) <- "Perc. 9 clus"
perc.var.9
```
```{r}
# Computing the percentage of variation accounted for. Ten clusters
(kmeans10.data <- kmeans(matstd.data,10,nstart = 10))
perc.var.10 <- round(100*(1 - kmeans10.data$betweenss/kmeans10.data$totss),1)
names(perc.var.10) <- "Perc. 6 clus"
perc.var.10
```

```{r}

Variance_List <- c(perc.var.2,perc.var.3,perc.var.4,perc.var.5,perc.var.6,perc.var.7,perc.var.8,perc.var.9,perc.var.10)

Variance_List
plot(Variance_List)
```
Insight: With k-means clustering, 5 clusters is the optimal option. 
As per the plot we can see that the curve of the slope starts to decrease significantly from 5-8 compared to all points before 5. Here we can also see, as we go on increasing the number of clusters the variance decreases. 
cluster 4 computes ~94% of variance
Cluster 5 computes ~96% of variance
Cluster 6 compute ~ 97% of variance

```{r}
optimal_num_clusters <- 7

#K-means clustering with the optimal number of clusters
kmeans_model <- kmeans(matstd.data, optimal_num_clusters, nstart = 10)

cluster_membership <- kmeans_model$cluster

# Print the cluster membership for each data point
print(cluster_membership)

# Scatter plot of the data with clusters colored by membership
plot(matstd.data[, 1], matstd.data[, 2], 
     col = kmeans_model$cluster, pch = 16, 
     xlab = "Variable 1", ylab = "Variable 2",
     main = "K-means Clustering")

# Adding cluster centers to the plot
points(kmeans_model$centers[, 1], kmeans_model$centers[, 2], col = 1:optimal_num_clusters, pch = 3, cex = 2)

```

```{r}
gap_stat <- clusGap(matstd.data, FUN = kmeans, nstart = 10, K.max = 10, B = 50)

fviz_gap_stat(gap_stat)


```
Insight: The optimal number of clusters k are 8. 


#  Optimal number of cluster according to k-means
```{r}
set.seed(123)
## Perform K-means clustering
km.res7 <- kmeans(matstd.data, 7, nstart = 25)  
# Visualize clusters
fviz_cluster(km.res7, data = matstd.data,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```
 
# Optimal number of clusters as permcomputer using k-means
```{r}
km.res8 <- kmeans(matstd.data, 8, nstart = 25)  
# Visualize clusters
fviz_cluster(km.res8, data = matstd.data,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```
# My optimal number of clusters according to hierarchical clustering
```{r}
km.res4 <- kmeans(matstd.data, 4, nstart = 25)  
# Visualize clusters
fviz_cluster(km.res4, data = matstd.data,  
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```

```{r}
# Perform Hierarchical Clustering
res.hc <- matstd.data %>% scale() %>% dist(method = "euclidean") %>%
  hclust(method = "ward.D2")  

# Change matstd.data to your dataset

# Visualize the Dendrogram
fviz_dend(res.hc, k = 4,  # Cut in four groups
          cex = 0.5,  # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE,  # color labels by groups
          rect = TRUE)
```

#Show a visualization of the cluster and membership using the first two Principal Components
```{r}
pca_result <- prcomp(data[, 5:7], scale. = TRUE)

# Assign clusters using k-means for example
k <- 4  # Number of clusters
set.seed(123)  # For reproducibility
cluster <- kmeans(pca_result$x[, 1:2], centers = k)$cluster

# Plot visualization
ggplot() +
  geom_point(data = as.data.frame(pca_result$x), aes(x = PC1, y = PC2, color = as.factor(cluster))) +
  scale_color_manual(values = c("blue", "red", "green", "orange")) +  # Adjust colors as needed
  labs(x = "Principal Component 1", y = "Principal Component 2", color = "Cluster") +
  theme_minimal()
```
The scatter plot visualizes the data points (countries) based on their positions defined by the first two Principal Components. Each data point is colored according to its cluster membership, as determined by the k-means clustering algorithm.The plot helps in identifying any patterns or separations between clusters based on the distribution of data points in the PCA space.We could see that there is stack of data points in between 0-2 Principal Component. 



